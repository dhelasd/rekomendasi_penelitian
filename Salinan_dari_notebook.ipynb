{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NIgdpN_4SzsK",
        "outputId": "d90a3a1b-2625-486a-8e78-26ca69e03cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df4d4ec83e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprefixspan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrefixSpan\u001b[0m \u001b[0;31m#modul khusus untuk prefixspan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m#baca file excel, csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;31m#untuk mengatur preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prefixspan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import warnings\n",
        "import unittest\n",
        "from prefixspan import PrefixSpan #modul khusus untuk prefixspan\n",
        "import pandas as pd #baca file excel, csv\n",
        "import nltk #untuk mengatur preprocessing\n",
        "from nltk.tokenize import word_tokenize #proses tokenize dilakukan oleh nltk\n",
        "from nltk.corpus import stopwords #stopwords dilakukan oleh nltk\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #stemmer dilakukan oleh sastrawi, kenapa tidak menggunakan nltk? karena untuk proses stemming di nltk itu bagus untuk bhs inggris (jadi harus ditranslate ke inggris, setelah didapatkan hasil di translate lagi ke indonesia) sehingga kurang pas untuk proses di nltk\n",
        "import re #untuk cleaning, jika butuh menghapus simbol2\n",
        "import string #modul untuk huruf  dan angka saat cleaning\n",
        "import os\n",
        "import time\n",
        "import resource\n",
        "import matplotlib.pyplot as plt #untuk membuat gambar, seperti word cloud, yg tampil langsung diaplikasi\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") #mematikan warning, karena warning bukan hanya muncul saat ada yang eror tetapi juga saat mengubah satu modul yang ada fitur maka warning akan muncul sehingga mengganggu\n",
        "pd.options.mode.chained_assignment = None #ada beberapa operasi yg jika kita lakukan itu versi kedepan akan dihapus maka akan dikeluarkan warning sehingga tidak ditampilkan. \n",
        "nltk.download('stopwords') #untuk download library\n",
        "nltk.download('punkt') #untuk download tanda henti \n",
        "factory = StemmerFactory()#mendefisi fungsi stemming\n",
        "stemmer = factory.create_stemmer() #membuat stemming\n",
        "listStopword =  set(stopwords.words('indonesian')) #load stopword, apa saja stopword nya dan menggunakan bahasa indonesia \n",
        "\n",
        "listtoberemoved = ['bengkulu', 'and', 'of', 'on','in','based','the','to','indonesia','thailand','from', 'for','berbasis','muara','bangkahulu','kota','di','untuk','lempuing'] #kata2 apa saja yg ingin dihapus karena tidak berpengaruh saat analisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wcYsjwKSzsR"
      },
      "outputs": [],
      "source": [
        "def memory_limit(): #untuk mengatur batasan memori\n",
        "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
        "    resource.setrlimit(resource.RLIMIT_AS, (get_memory() * int(1024 / 2), hard))\n",
        "\n",
        "def get_memory(): #untuk mengambil data memori\n",
        "    with open('/proc/meminfo', 'r') as mem:\n",
        "        free_memory = 0\n",
        "        for i in mem:\n",
        "            sline = i.split()\n",
        "            if str(sline[0]) in ('MemFree:', 'Buffers:', 'Cached:'):\n",
        "                free_memory += int(sline[1])\n",
        "    return free_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwhGlWFUSzsS"
      },
      "outputs": [],
      "source": [
        "def read_data(x): #membaca data\n",
        "    XL = pd.ExcelFile(os.path.join(\"data\",\"data_mod.xlsx\")) #membaca file\n",
        "    sheet = XL.sheet_names[x] #dari file ada sheet apa saja, sheet tergantung urutan (pada python itu mulai dari 0-5, 0 itu LPPM)\n",
        "    data = pd.read_excel(XL, sheet)\n",
        "    data = data[~data['Topik'].isnull()] #memulai cleaning, jika tidak ada topik penelitian pada data tersebut maka akan ditampilkan lagi\n",
        "    data['Tahun'] = data['Tahun'].astype(int) #data tahun diganti jadi integer \n",
        "    data = data.reset_index(drop=True) #direset index dan di drop, dan kemudian agar data yg tidak ada topik tadi terhapus, dan nomornya bisa kembali sesuai dengan urutan\n",
        "    data = data[data.columns.tolist()[:-3]].join(data[data.columns.tolist()[-2:]]).join(data[data.columns.tolist()[-3]]) #untuk menggeser supaya kolom lebih urut saat data penelitinya hanya sedikit \n",
        "    return data, sheet\n",
        "\n",
        "def preprocessing(data, listtoberemoved): #memulai proses preprocessing\n",
        "    df = pd.DataFrame(data['Judul'],columns=['Judul']).copy() #membuat data frame awal yang isi nya judul\n",
        "    cleaned = [] #membuat list baru berjudul cleaned\n",
        "    for n in df['Judul'].values:\n",
        "        n = n.lower() #proses case folding\n",
        "        n = re.sub(r':', '', n) #proses cleaning\n",
        "        n = re.sub(r'‚Ä¶', '', n) #proses cleaning\n",
        "        n = re.sub(r'[^\\x00-\\x7F]+',' ', n) #proses cleaning\n",
        "        n = re.sub('[^a-zA-Z]', ' ', n) #proses cleaning\n",
        "        n = re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",n) #proses cleaning\n",
        "        n = re.sub(\"(\\\\d|\\\\W)+\",\" \",n) #proses cleaning\n",
        "        n = re.sub(r'â', '', n) #proses cleaning\n",
        "        n = re.sub(r'€', '', n) #proses cleaning\n",
        "        n = re.sub(r'¦', '', n) #proses cleaning\n",
        "        cleaned.append(n) #setelah dicleaning, data ditambahkan dilist yg nama nya cleaning\n",
        "    df['cleaned'] = cleaned  #setelah list cleaning terbentuk, maka membuat satu kolom didata frame df yg nama kolomnya cleaned, dan data clean akan dimasukkan\n",
        "\n",
        "    tokenized = [] #memulai list baru berjudul token\n",
        "    for n in cleaned: #mentokenize data yang berasal dari proses cleaning\n",
        "        n = word_tokenize(n) \n",
        "        tokenized.append(n)\n",
        "    df['tokenized'] = [', '.join(n) for n in tokenized] #dan hasil tokenize dimasukkan pada kolom yang baru bernama tokinezed\n",
        "\n",
        "    removed = [] #memulai list baru berjudul removed\n",
        "    for ts in tokenized: #menremoved data stopwords dari proses tokenized\n",
        "        n = []\n",
        "        for t in ts:\n",
        "            if t not in listtoberemoved and t not in listStopword and t not in string.punctuation: #selain meremove stopwords, listoberemoved yg sebelumnya diatur jga dihapus\n",
        "                n.append(t)\n",
        "        removed.append(n)\n",
        "    df['removed'] = [', '.join(n) for n in removed] #hasil removed dimasukkan ke kolom baru bernama removed\n",
        "\n",
        "    stemmed = [] #memulai list baru berjudul stemmed\n",
        "    for n in removed: #proses stemming dilakukan dari data removed\n",
        "        n = ' '.join(n)\n",
        "        n = stemmer.stem(n)\n",
        "        n = n.split(' ')\n",
        "        stemmed.append(n)\n",
        "    df['stemmed'] = [' '.join(n) for n in stemmed] #hasil stemming dimasukkan ke kolom baru bernama stemmed\n",
        "    return df, stemmed\n",
        "\n",
        "def mining(data,stemmed, ms, mp, mnp): #mulai data mining\n",
        "\n",
        "    dx = [n for n in [a + b + c for a,b,c in zip(stemmed,data['Keyword'].str.split(\",\").values.tolist(),data['Topik'].str.split(\",\").values.tolist())]] #hasil dari proses stemming, digabungkan dengan keyword dan topik, sehingga pattern nya nambah\n",
        "    ps = PrefixSpan(dx) #data sebelumnya, dimasukkan ke proses prefixspan. untuk mendapatkan prefixspan diatur min freq=2(yg pattern muncul hanya sekali, tidak dihitung), max pattern=10(yg pattern>10 tidak akan muncul), min pattern=2(<2 tidak dihitung)\n",
        "    pf_results = pd.DataFrame(ps.frequent(ms), columns=['freq','sequence']) #untuk setting min freq\n",
        "    \n",
        "    pf_results['sequence'] = [', '.join(n) for n in pf_results['sequence'].values.tolist()] #untuk menggabungkan list pd proses preprocessing, karena sebelumnya hanya breket atau tanda kurung menjadi tabel\n",
        "    pf_results = pf_results[[len(n)<=mp for n in pf_results['sequence'].str.split(\",\").values.tolist()]] #memfilter max patt\n",
        "    pf_results = pf_results[[len(n)>=mnp for n in pf_results['sequence'].str.split(\",\").values.tolist()]].sort_values(by='freq',ascending=False).reset_index(drop=True) #memfilter min patt\n",
        "    return pf_results #memunculkan hasil \n",
        "\n",
        "def run(seq,ms,mp,mnp): #merunning fungsi diatas\n",
        "    data, sheet= read_data(seq)\n",
        "    df, stemmed = preprocessing(data, listtoberemoved)\n",
        "    pf = mining(data, stemmed, ms=ms, mp=mp, mnp=mnp)\n",
        "    return pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joenZTrxSzsU"
      },
      "outputs": [],
      "source": [
        "memory_limit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3uhEMyrSzsU"
      },
      "outputs": [],
      "source": [
        "report = []\n",
        "for i,jur in enumerate([\"Informatika\",\"Sipil\",\"Mesin\",\"Elektro\",\"Arsitektur\",\"Sistem Informasi\"]):\n",
        "    for j in range(2,7):\n",
        "        start_time = time.time()\n",
        "        if jur==\"Mesin\" and j == 2:\n",
        "            continue\n",
        "        else:\n",
        "            try:\n",
        "                run(i+1,j,10,3)\n",
        "            except MemoryError:\n",
        "                print(\"Memory Insufficient\")\n",
        "                continue\n",
        "        # print(jur,j,time.time() - start_time, get_memory())\n",
        "        report.append([jur,j,time.time() - start_time, get_memory()])\n",
        "\n",
        "df_report = pd.DataFrame(report, columns=[\"data\",\"min support\",\"running time (s)\", \"memory usage (kB)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpsij4SLSzsV"
      },
      "outputs": [],
      "source": [
        "df_report[[\"data\",\"min support\",\"running time (s)\"]][df_report[\"data\"]==\"Mesin\"].set_index(\"min support\").plot.bar(rot=0, subplots=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95htSMq3SzsW"
      },
      "outputs": [],
      "source": [
        "min(df_report[\"running time (s)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEhVyF_2SzsX"
      },
      "outputs": [],
      "source": [
        "for h in [\"running time (s)\",\"memory usage (kB)\"]:\n",
        "    fig, axs = plt.subplots(2, 3,figsize = (20,15), sharex=True)\n",
        "    for i,jur in enumerate([\"Informatika\",\"Sipil\",\"Mesin\",\"Elektro\",\"Arsitektur\",\"Sistem Informasi\"]):\n",
        "        axs[i//3, i%3].bar(\n",
        "            df_report[[\"data\",\"min support\",h]][df_report[\"data\"]==jur].set_index(\"min support\").index.astype(str), \n",
        "            df_report[[\"data\",\"min support\",h]][df_report[\"data\"]==jur].set_index(\"min support\")[h].values\n",
        "        )\n",
        "        axs[i//3, i%3].set_title(jur)\n",
        "        axs[i//3, i%3].set_ylim([min(df_report[h]), max(df_report[h]) + max(df_report[h])*0.01])\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.set(xlabel='minimum support', ylabel=h)\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.label_outer()\n",
        "    \n",
        "    fig.suptitle(f'Prefix Span Performance based on Min. Support & {h}', fontsize=16, y=0.92)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS6aX45YSzsY"
      },
      "outputs": [],
      "source": [
        "for h in [\"running time (s)\",\"memory usage (kB)\"]:\n",
        "    fig, axs = plt.subplots(2, 3,figsize = (20,15), sharex=True)\n",
        "    fig = plt.figure(figsize = (8,6))\n",
        "    for i,jur in enumerate([\"Informatika\",\"Sipil\",\"Mesin\",\"Elektro\",\"Arsitektur\",\"Sistem Informasi\"]):\n",
        "        plt.plot(\n",
        "            df_report[[\"data\",\"min support\",h]][df_report[\"data\"]==jur].set_index(\"min support\").index.astype(str), \n",
        "            df_report[[\"data\",\"min support\",h]][df_report[\"data\"]==jur].set_index(\"min support\")[h].values,\n",
        "            label = jur\n",
        "        )\n",
        "        axs[i//3, i%3].set_title(jur)\n",
        "        axs[i//3, i%3].set_ylim([min(df_report[h]), max(df_report[h]) + max(df_report[h])*0.01])\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.set(xlabel='minimum support', ylabel=h)\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.label_outer()\n",
        "    plt.legend(loc=\"center right\")\n",
        "    fig.suptitle(f'Prefix Span Performance based on Min. Support & {h}', fontsize=12, y=0.95)\n",
        "    fig.show(marker='s')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('2021-098-text-information-retrieval--UlNGMHpK')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "407a95d11ee9317e5cec2570e302b5c822b852211f2851fc665982f2d473bc22"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}